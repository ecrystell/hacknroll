{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n",
      "Start with white yarn\n",
      "R1. 6sc in mr, sl st and ch1 in the first st\n",
      "R2. 6inc, sl st and ch1 in the first st\n",
      "R3. (sc, inc) x6, sl st and ch1 in the first st\n",
      "R4. (2sc, inc) x6, sl st and ch1 in the first st\n",
      "R5. (3sc, inc) x6, sl st and ch1 in the first st\n",
      "R6-R9. sc all around (4 rounds)\n",
      "[6]\n",
      "[12]\n",
      "[18]\n",
      "[24]\n",
      "[30]\n",
      "[30]\n",
      "R10. (3sc, dec) x6, sl st and ch1 in the first st [24]\n",
      "R11. (2sc, dec) x6, sl st and ch1 in the first st [18]\n",
      "Please go to next\n",
      "page to continue\n",
      "@moooorumi\n",
      "Save for later\n"
     ]
    }
   ],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "\n",
    "    for text in texts:\n",
    "        out = text.description\n",
    "        break\n",
    "\n",
    "    return out\n",
    "        # vertices = [\n",
    "        #     f\"({vertex.x},{vertex.y})\" for vertex in text.bounding_poly.vertices\n",
    "        # ]\n",
    "\n",
    "        # print(\"bounds: {}\".format(\",\".join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "print(detect_text('static/images/photo_6077768651492148108_y.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the AI Platform with your project and region\n",
    "project_id = 'ocr-test-448210'\n",
    "region = 'us-central1'  # e.g., 'us-central1'\n",
    "\n",
    "aiplatform.init(project=project_id, location=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19444\\1343856854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0minstances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic import PredictionServiceClient\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project='ocr-test-448210',  # Replace with your project ID\n",
    "    location=\"us-central1\"  # Pre-trained models are typically hosted in 'us-central1'\n",
    ")\n",
    "\n",
    "# Define the model name (pre-trained model)\n",
    "model_name = \"text-bison\"\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = PredictionServiceClient()\n",
    "\n",
    "project_id = \"ocr-test-448210\"\n",
    "location = \"us-central1\"  # Replace with the model's region\n",
    "model_name = \"text-bison\"  # Example pre-trained model name\n",
    "\n",
    "# Construct the endpoint and model resource path\n",
    "endpoint = f\"projects/{project_id}/locations/{location}/publishers/google/models/{model_name}\"\n",
    "\n",
    "\n",
    "# Specify the endpoint\n",
    "endpoint = f\"projects/{aiplatform.initializer.global_config.project}/locations/us-central1/publishers/google/models/{model_name}\"\n",
    "\n",
    "# Define the input for prediction\n",
    "instances = [{\"content\": \"What is the capital of France?\"}]\n",
    "parameters = {\"temperature\": 0.7, \"maxOutputTokens\": 256}\n",
    "\n",
    "# Send the request\n",
    "response = model.predict(\n",
    "    name=endpoint,\n",
    "    instances=instances,\n",
    "    parameters=parameters\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(\"Generated text:\", response.predictions[0]['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
